#---------------------------------------------------------------
# Takes training data generated by SAS project:
# Modeling\ACI Risk Segmentation\2022\ACI Risk Tiers 2022 Update
#
# Developed for ACI classification of SLM loans
#---------------------------------------------------------------


print("importing packages and data...")

import pandas as pd
import numpy as np
from imblearn.over_sampling import RandomOverSampler
from collections import Counter
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.model_selection import (
    train_test_split,
    cross_val_score,
    GridSearchCV,
)
from sklearn import metrics, tree
from matplotlib import pyplot as plt
import graphviz
import os

pd.options.mode.chained_assignment = (
    None  # dismisses pandas warnings for chained assignments; default='warn'
)

# pointing to Graphviz bin file so pycharm doesn't get confused and error out
os.environ["PATH"] += (
    os.pathsep
    + "C:/Users/e80783/OneDrive - Sallie Mae/Mike/python/packages/Graphviz/bin"
)

# define graph export function for quick calls
def gviz(graph_name):
    export_graphviz(
        clf,
        out_file=graph_name + ".dot",
        feature_names=predictors,
        impurity=True,
        filled=True,
        proportion=True,
    )
    with open(graph_name + ".dot") as f:
        dot_graph = f.read()
    g = graphviz.Source(dot_graph)
    g.view(filename=graph_name)


# read in training data (convert SAS to CSV because pandas likes CSVs
#pima = pd.read_sas("rrps_tu_final_202206.sas7bdat", encoding="iso-8859-1")
#print("# of observations: ", len(pima.index))
#pima.to_csv("mod_training.csv", index=False, header=True)
#print("CSV saved")


cols = [
    "DLQ2P",
    "FORB_PCNT_USE",
    "BOR_TRD_36_MO_WST_NUM",
    "FICO",  # used to be named "RRP_RCNT_FICO"
    # "BOR_TRD_30_DAY_RATG_CNT",
    "STDDEV_PI_V1",
    "RRP_RCNT_BR_FICO",
    "FORB_USED_BEFR_ACI",
    "BOR_HIGH_CRDT_TO_LMT_PCT_DLT",
    "PMT_RCVE_6MO_BEFR_ACI",
    "DISB_AMT",
    "TARGET",
]

df = pd.read_csv("mod_training.csv", usecols=cols)


# imputing missing values. Chosen method is means by 50pt FICO bucket
buckets = np.array(range(300, 851, 50))
print(buckets)
print("count of NULL values before imputation\n", df.isnull().sum())
df["FICO_BUCKET"] = pd.cut(df.FICO, buckets)

df = df.groupby("FICO_BUCKET").transform(lambda x: x.fillna(x.mean()))
print("count of NULL values after imputation\n", df.isnull().sum())

# certain columns can only be integers, so we round them
round_cols = [
    "FICO",
    "RRP_RCNT_BR_FICO",
    "BOR_TRD_36_MO_WST_NUM",
    "BOR_HIGH_CRDT_TO_LMT_PCT_DLT",
]

for c in round_cols:
    df[c] = df[c].round(decimals=0)

# drop any remaining missing values (there shouldn't be any but just in case)
df = df.dropna()
print("# of non-missing observations: ", len(df.index))


# defining predictors and target variable
predictors = cols[:-1]  # everything except target
print("predictor columns: ", predictors)

x = df[predictors]
y = df.TARGET


# oversample defaults to get a 50/50 split between target = 0 and target = 1
print(Counter(y))
oversample = RandomOverSampler(sampling_strategy="minority")
x_over, y_over = oversample.fit_resample(x, y)
print(Counter(y_over))

rs = 25  # random state; basically the seed for the "random" data splits

# 60/40 split between training and testing data
x_train, x_test, y_train, y_test = train_test_split(
    x_over, y_over, test_size=0.6, random_state=rs
)


# preliminary tree with no pruning or limitations (other than max depth)
print("Fitting preliminary tree...")
clf = DecisionTreeClassifier(
    random_state=rs,
    max_depth=6,  # arbitrary but keeps the preliminary tree from exploding
)
clf = clf.fit(x_train, y_train)
y_pred = clf.predict(x_test)
print("Visualizing tree...")
gviz("prelim_tree")


# pre-pruning: grid search to find the best tree criterion
print("Using grid search...")
params = {
    "criterion": ["gini", "entropy"],
    "splitter": ["best"],
    "max_depth": range(3, 6, 1),
    "min_samples_leaf": range(1000, 6000, 2500),
    "min_samples_split": range(1000, 10000, 3000),
}
grid_search = GridSearchCV(estimator=clf, param_grid=params, cv=5, n_jobs=-1)
grid_search.fit(x_train, y_train)
result = grid_search.best_params_
print("Best parameters: ", result)

# assign best parameters to variables
criterion = result["criterion"]
md = result["max_depth"]
msl = result["min_samples_leaf"]
mss = result["min_samples_split"]

print("Fitting pre-pruned tree...")
clf = DecisionTreeClassifier(
    criterion=criterion,
    max_depth=md,
    min_samples_leaf=msl,
    min_samples_split=mss,
    random_state=rs,
)
clf = clf.fit(x_train, y_train)
# y_pred = clf.predict(x_test)

print("Visualizing pre-pruned tree...")
gviz("pre_pruned")


# post-pruning; refining the tree using cost-complexity analysis
path = clf.cost_complexity_pruning_path(x_train, y_train)
ccp_alphas = (
    path.ccp_alphas
)  # cost-complexity pruning; the trade-off between accuracy and complexity
ccp_alphas = ccp_alphas[:-1]
clf_dts = []

print("Generating trees for CCP...")
for ccp_alpha in ccp_alphas:
    clf = DecisionTreeClassifier(
        random_state=rs,
        ccp_alpha=ccp_alpha,
        criterion=criterion,
        max_depth=md,
        min_samples_leaf=msl,
        min_samples_split=mss,
    )
    clf.fit(x_train, y_train)
    clf_dts.append(clf)
print(len(clf_dts), "trees generated")

train_scores = [clf.score(x_train, y_train) for clf in clf_dts]
test_scores = [clf.score(x_test, y_test) for clf in clf_dts]

# plotting training vs test scores accuracy against CCP
fig, ax = plt.subplots()
ax.set_xlabel("alpha")
ax.set_ylabel("accuracy")
ax.set_title("accuracy vs alpha for training and test datasets")
ax.plot(ccp_alphas, train_scores, marker="o", label="train", drawstyle="steps-post")
ax.plot(ccp_alphas, test_scores, marker="o", label="test", drawstyle="steps-post")
ax.legend()
plt.show()


print("Fitting post-pruned tree...")
#  picking a CCP such that we don't lose much accuracy but wind up with a manageable tree
clf = DecisionTreeClassifier(
    criterion=criterion,
    max_depth=md,
    min_samples_leaf=msl,
    min_samples_split=0.2,  # mss,
    random_state=rs,
    ccp_alpha=0.00144,
)
clf = clf.fit(x_train, y_train)

y_pred = clf.predict(x_test)
print("Accuracy", metrics.accuracy_score(y_test, y_pred))

print("Visualizing post-pruned tree...")
gviz("post_pruned")

#  neater way to plot
plt.figure(figsize=(20, 12))
tree.plot_tree(
    clf, feature_names=predictors, rounded=True, filled=True, proportion=True
)
plt.show()

print("Done")
